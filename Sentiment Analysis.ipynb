{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab737fd7-c3df-44fc-8b35-48c3eb95926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a597afb-be5c-4e14-8405-e6cdd4c17d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(file_path, label):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            text = line.strip()\n",
    "            if text:\n",
    "                data.append({\"text\": text, \"label\": label})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b99dec-3940-4537-a2f8-726ded870975",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_data += load_train_data(\"SA2016-training_data/SA-training_negative.txt\", 0)\n",
    "train_data += load_train_data(\"SA2016-training_data/SA-training_neutral.txt\", 1)\n",
    "train_data += load_train_data(\"SA2016-training_data/SA-training_positive.txt\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c238035-3f76-4ae1-8b45-11736a50a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905e580-deb4-4bfe-a4a2-a33909067179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(file_path):\n",
    "    test_samples = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    for i in range(0, len(lines), 2):\n",
    "        text = lines[i]\n",
    "        true_label = lines[i+1] if (i+1) < len(lines) else None\n",
    "        test_samples.append({\"text\": text, \"label\": true_label})\n",
    "    return test_samples\n",
    "\n",
    "test_data = load_test_data(\"SA2016-TestData-Ans/test_raw_ANS.txt\")\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e82c7-9411-4e67-bfb8-0eade3e1caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\", use_fast=False)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "label_mapping = {\"NEG\": 0, \"NEU\": 1, \"POS\": 2}\n",
    "\n",
    "def convert_label(example):\n",
    "    if isinstance(example[\"label\"], str):\n",
    "        example[\"label\"] = label_mapping[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "tokenized_test = tokenized_test.map(convert_label)\n",
    "\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f056f96-a2d7-4bd1-ab6d-ede6c810fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"vinai/phobert-base-v2\", num_labels=3)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base-v2\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed9e1a-34cf-4bae-88e8-9d38073f2039",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=20,              \n",
    "    learning_rate=1e-5,       \n",
    "    warmup_steps=500,                  \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",            \n",
    "    load_best_model_at_end=True,       \n",
    "    metric_for_best_model=\"eval_accuracy\", \n",
    "    greater_is_better=True,          \n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1fb11-5b5b-4d9d-a92a-4e4e3b9a9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a742299b-af25-4bf1-8578-b80667ed5179",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,   \n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a16aa-99d2-4974-ab29-7de05dcf8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60d325-ee1d-4106-bfb0-635604490757",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval_result = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"Test Accuracy:\", test_eval_result[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824103b1-f12d-4339-82c3-9d0c22a84e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = \"models/phobert-base-v2-3\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_folder)\n",
    "tokenizer.save_pretrained(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bffea4-f84b-453e-a520-d4dc612b254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_folder = \"models/phobert-base-v2-2\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_folder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d76e0c-6e10-40df-a355-4d23f1ae42e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "test_eval_result = trainer.evaluate(eval_dataset=tokenized_test)\n",
    "print(\"Test Accuracy:\", test_eval_result[\"eval_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96624577-b3fa-4316-82d3-180d7ac32b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
